# 평가 기준 템플릿 허브: 최종 통합 제안서

## 1. 서론

본 문서는 CR 프로젝트의 '평가 기준 템플릿 허브 사이트' 구축을 위해 제안된 여러 아이디어를 종합적으로 분석하고, 각 제안의 강점을 극대화한 최종 통합안을 제시합니다. 이 제안서는 **빠른 실행 가능성**, **평가 품질 확보**, **장기적 지속가능성**이라는 세 가지 핵심 목표의 균형을 맞추는 데 중점을 둡니다.

---

## 2. 핵심 설계 철학: '교육적 유도'와 '선택권 존중'의 조화

프로젝트의 성공은 양질의 평가 데이터 확보에 달려있습니다. 이를 위해 본 통합안은 **'평가 품질 우선'** 원칙을 채택하되, 사용자의 반감을 사지 않는 정교한 접근 방식을 제안합니다.

> **"사용자를 강제하는 것이 아니라, 명확한 데이터와 교육적 가이드를 통해 최적의 선택을 하도록 돕는다."

- **투명한 정보 제공**: 왜 특정 AI가 더 나은지 객관적인 데이터(성능 점수, 기능 차이)를 투명하게 공개합니다.

- **강력한 유도 (Nudging)**: 시각적 강조(버튼 색상, 크기)와 기본값 설정을 통해 사용자가 자연스럽게 최적의 경로를 선택하도록 유도합니다.

- **궁극적 선택권 보장**: 사용자가 경고를 인지한 상태에서 비최적 환경을 선택할 자유를 보장하되, 그에 따른 한계점을 명확히 안내합니다.

- **교육적 접근**: 사용자가 AI 리터러시를 높이고, 프로젝트의 파트너로서 성장할 수 있도록 돕는 교육적 콘텐츠를 제공합니다.

---

## 3. 최종 제안 플로우 (The Integrated Flow)

각 제안의 장점을 결합한 최종 사용자 플로우는 다음과 같습니다.

```
[시작]
 ↓
1. 기사 URL 입력 및 AI 환경 파악
   - "분석할 기사 주소를 입력해주세요."
   - "현재 주로 사용하는 AI 서비스는 무엇인가요?" (선택: Claude, ChatGPT, Gemini, 기타, 없음) (친구 D)
 ↓
2. 기사 자동 분석 및 유형 확인
   - (로딩) "기사를 분석하고 있습니다..."
   - "AI가 이 기사를 [스트레이트 뉴스]로 분석했습니다. 정확한가요?" (친구 C)
   - [네, 맞습니다] [아니요, 직접 수정할게요 → 유형 선택] (친구 C)
 ↓
3. 최적 환경 추천 및 성능 비교
   - (분석 결과 + 추천 AI 강조 표시) (친구 B, D)
   - ┌───────────────────────────────────┐
   - │ 📰 분석 결과: 스트레이트 뉴스, 사실 확인 중심 │
   - │ ✅ 권장 AI: Claude 3.5 Sonnet (예상 품질: ★★★★★) │
   - │ [💎 Claude로 평가하기] ← 주 버튼 (친구 B) │
   - │                                           │
   - │ 현재 사용 AI: ChatGPT-4o (예상 품질: ★★★★☆) │
   - │ [내 AI로 계속하기] [상세 비교 보기] (친구 D) │
   - └───────────────────────────────────┘
 ↓
4. 차별화된 템플릿 및 가이드 제공 (친구 D)
   - (A) 'Claude로 평가하기' 선택 시:
     - ✅ 최고 품질 템플릿 세트 (최적화된 프롬프트, 상세 가이드, 예시 리포트)
   - (B) '내 AI로 계속하기' 선택 시:
     - ⚠️ "선택하신 환경은 최적 성능이 아닐 수 있습니다."
     - ✅ 기본 템플릿 + 해당 AI용 가이드 + **한계점 및 보완 방법 안내 문서**
 ↓
5. 다운로드 완료 및 피드백 요청
   - "다운로드가 완료되었습니다."
   - "평가 완료 후, 결과를 이곳에 제출하여 프로젝트에 기여해주세요!" (구글 폼 링크) (친구 C)
   - "다른 평가자들의 결과가 궁금하다면? [커뮤니티 방문하기]" (친구 D)
[종료]
```

---

## 4. 단계별 개발 로드맵 (Phased Approach)

모든 기능을 한 번에 구현하는 것은 비효율적입니다. 따라서, **'빠른 출시 → 점진적 고도화'** 전략을 채택하여 3단계로 나누어 개발할 것을 제안합니다.

### **Phase 1: MVP - 핵심 기능 구현 (4~6주)**

**목표**: 가장 기본적인 기능만으로 빠르게 출시하여 사용자 피드백을 수집하고 시장성을 검증합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **기사 URL 입력 및 스크래핑** | 사용자가 URL을 입력하면 기사 본문을 추출합니다. |  |
| **기사 유형 자동 분류 (LLM API)** | 저비용 LLM API(Claude Haiku 등)를 활용해 기사 유형을 분류합니다. |  |
| **기사 유형 수동 수정** | AI의 분류가 틀렸을 경우 사용자가 직접 수정할 수 있는 기능을 제공합니다. | **** |
| **AI 서비스 선택** | 사용자가 자신이 사용할 AI를 드롭다운 메뉴에서 직접 선택합니다. |  |
| **기본 템플릿 제공** | 선택된 (기사 유형 x AI 서비스) 조합에 맞는 기본 템플릿과 가이드를 제공합니다. |  |
| **피드백 수집 (외부 폼)** | 구글 폼 또는 Airtable을 연동하여 사용자의 평가 결과를 수집합니다. | **** |

**기술 스택 (Phase 1)**:

- **프론트엔드**: Next.js 또는 SvelteKit

- **백엔드**: Python (FastAPI) + BeautifulSoup/Requests

- **기사 분류**: 외부 LLM API (Anthropic/OpenAI)

- **데이터 관리**: JSON 파일 (템플릿 매핑), Google Forms (피드백)

### **Phase 2: 품질 강화 (추가 6~8주)**

**목표**: MVP에서 수집된 데이터를 바탕으로, 평가 품질을 높이기 위한 유도 장치를 도입합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **AI 성능 데이터베이스 구축** | 각 AI 모델의 기사 유형별 예상 성능 점수를 관리하는 DB를 구축합니다. | **** |
| **최적 환경 추천 시스템** | 기사 분석 결과를 바탕으로 최적의 AI를 추천하고, 시각적으로 강조(주 버튼)합니다. |  |
| **성능 비교 데이터 제시** | 사용자가 다른 AI를 선택하려 할 때, 예상 성능 점수와 한계점을 명확히 보여줍니다. |  |
| **사용자 선택 데이터 수집** | 사용자가 어떤 경로(추천/비추천)를 선택하는지 데이터를 수집하여 분석합니다. |  |

**기술 스택 (Phase 2 추가)**:

- **데이터베이스**: PostgreSQL 또는 Supabase (AI 성능 데이터, 사용자 선택 로그)

- **프론트엔드**: Chart.js (성능 비교 시각화)

### **Phase 3: 생태계 고도화 (추가 8~12주)**

**목표**: 사용자를 단순 이용자에서 프로젝트의 적극적인 참여자로 전환하고, 장기적인 데이터 선순환 구조를 완성합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **사전 환경 감지** | 1단계에서 사용자의 AI 환경을 먼저 파악하여 더욱 개인화된 추천을 제공합니다. | ** ** |
| **차별화된 템플릿 패키지** | 최적 환경 사용자에게는 비디오 가이드, 체크리스트 등 'VIP 패키지'를, 비최적 사용자에게는 '보완 가이드'를 제공합니다. | **** |
| **교육적 콘텐츠 허브** | AI 서비스별 장단점, 비용 비교, CR 프로젝트의 중요성 등을 설명하는 별도 페이지를 구축합니다. |  |
| **A/B 테스트 프레임워크** | 어떤 유도 문구나 UI가 더 효과적인지 지속적으로 실험하고 개선합니다. |  |

**기술 스택 (Phase 3 추가)**:

- **프론트엔드**: Framer Motion (애니메이션), Chakra UI (고급 컴포넌트)

- **백엔드**: 템플릿 동적 생성 엔진 (Jinja2 등)

- **분석**: Mixpanel 또는 자체 분석 시스템 (A/B 테스트, 사용자 행동 분석)

---

## 5. 결론 및 기대 효과

본 통합 제안서는 **'실용적 시작'**과 **'야심찬 비전'**을 결합한 로드맵입니다. 현실적인 MVP 제안으로 빠르게 시작하되, 강력한 품질 관리 및 사용자 참여 전략을 단계적으로 도입함으로써, CR 프로젝트의 핵심 목표인 **'신뢰할 수 있는 시민 주도 언론 비평 생태계 구축'**을 가장 효과적으로 달성할 수 있을 것입니다.

**기대 효과**:

- **단기**: 빠른 서비스 출시로 초기 사용자 확보 및 데이터 축적 시작

- **중기**: 평가 품질의 상향 평준화 및 플랫폼 신뢰도 제고

- **장기**: 사용자의 자발적 참여와 데이터 기여를 통한 지속가능한 성장 및 언론 개혁 동력 확보


=========================


# 템플릿 허브 사이트: 실행 가이드

## 🛠️ Phase 1 MVP 구현 가이드 (즉시 실행 가능)

### 필수 기능 체크리스트

- [ ] **기사 스크래핑 모듈** (Python + BeautifulSoup)

- [ ] **기사 유형 자동 분류** (LLM API: Claude Haiku 또는 GPT-4o-mini)

- [ ] **기사 유형 수동 수정 UI** (드롭다운 메뉴)

- [ ] **AI 서비스 선택 UI** (Claude, ChatGPT, Gemini, 기타)

- [ ] **템플릿 매핑 시스템** (JSON 파일: `{기사유형: {AI: {template_path, guide_path}}}`)

- [ ] **다운로드 기능** (md, txt 파일)

- [ ] **피드백 수집** (구글 폼 링크 연동)

### 권장 기술 스택 (Phase 1)

```
프론트엔드:
- Next.js (React) + TailwindCSS
- 또는 SvelteKit + TailwindCSS

백엔드:
- Python FastAPI
- BeautifulSoup + Requests (스크래핑)
- Anthropic/OpenAI API (기사 분류)

데이터:
- JSON 파일 (템플릿 매핑)
- Google Forms (피드백 수집)

배포:
- Vercel (프론트엔드)
- Railway/Render (백엔드)
```

### 예상 개발 일정 (4-6주)

| 주차 | 작업 |
| --- | --- |
| **1주차** | 프로젝트 셋업, 기사 스크래핑 모듈 개발 |
| **2주차** | 기사 유형 분류 API 연동, 프론트엔드 UI 개발 |
| **3주차** | 템플릿 매핑 시스템, 다운로드 기능 구현 |
| **4주차** | 피드백 폼 연동, 통합 테스트 |
| **5-6주차** | 버그 수정, 베타 테스트, 출시 |

---

## 📊 성공 지표 (KPI)

### Phase 1 목표

- **사용자 수**: 100명 이상

- **템플릿 다운로드 수**: 200건 이상

- **피드백 제공률**: 10% 이상

- **기사 유형 분류 정확도**: 70% 이상

### Phase 2 목표

- **최적 환경 선택률**: 60% 이상

- **평가 품질 점수**: 평균 80점 이상

- **재방문율**: 30% 이상

### Phase 3 목표

- **최적 환경 선택률**: 75% 이상

- **평가 품질 점수**: 평균 85점 이상

- **재방문율**: 50% 이상

- **커뮤니티 참여율**: 20% 이상

---

## ⚠️ 주요 리스크 및 대응 방안

### 리스크 1: 기사 스크래핑 차단

**대응**: User-Agent 변경, 프록시 서버, Puppeteer(동적 렌더링) 활용

### 리스크 2: 기사 유형 분류 정확도 낮음

**대응**:

- Phase 1: 사용자 수동 수정 기능으로 보완

- Phase 2: 수집된 정답 데이터로 프롬프트 개선

- Phase 3: 자체 ML 모델 학습

### 리스크 3: AI 성능 데이터 부족

**대응**:

- Phase 1: AI 성능 데이터 없이 출시 (사용자 선택만)

- Phase 2: 소규모 베타 테스트로 성능 데이터 수집

- Phase 3: 실제 데이터 기반 성능 DB 구축

### 리스크 4: 사용자 피드백 저조

**대응**:

- 피드백 제공 시 인센티브 제공 (예: 우수 평가자 배지)

- 피드백 과정 최대한 간소화 (구글 폼 1페이지)

- 피드백의 중요성을 교육적으로 설명

---

## 🚀 즉시 실행 가능한 Next Steps

### 1주일 내 실행 가능한 작업

1. **프로젝트 저장소 생성** (GitHub)

1. **기술 스택 확정** (Next.js vs SvelteKit, FastAPI vs Node.js)

1. **기사 스크래핑 프로토타입 개발** (3-5개 주요 언론사 대상)

1. **LLM API 계정 생성** (Anthropic 또는 OpenAI)

1. **기사 유형 분류 프롬프트 초안 작성**

1. **템플릿 파일 구조 설계** (기사 유형 × AI 서비스 매트릭스)

### 2주일 내 실행 가능한 작업

1. **프론트엔드 와이어프레임 작성** (Figma 등)

1. **백엔드 API 엔드포인트 설계**

1. **기본 UI 컴포넌트 개발** (URL 입력, 로딩, 결과 표시)

1. **템플릿 샘플 파일 작성** (최소 3개 기사 유형 × 3개 AI)

---

## 📚 참고 문서

본 분석 과정에서 작성된 상세 문서들:

1. **`proposals_comparison.md`**: 4가지 제안의 상세 비교 분석

1. **`final_template_hub_plan.md`**: 최종 통합 제안서 (전체 플로우, 로드맵)

1. **`template_hub_ux_ui_analysis.md`**: 원안에 대한 UX/UI 평가 및 개선안

1. **`technical_implementation_plan.md`**: 원안 기반 기술 구현 방안

---

## 💡 최종 권고사항

**"완벽한 시스템을 처음부터 만들려 하지 말고, 작게 시작하여 빠르게 배우고 개선하라."**

1. **Phase 1 MVP에 집중**: 4-6주 내 출시 목표

1. **사용자 피드백 최우선**: 초기 사용자의 의견을 적극 수렴

1. **데이터 기반 의사결정**: Phase 2 이후는 실제 데이터를 바탕으로 개선

1. **품질과 속도의 균형**: 빠른 출시와 평가 품질 확보를 동시에 추구



이러한 접근은 CR 프로젝트의 핵심 가치인 **'시민 참여'**와 **'데이터 기반 객관성'**을 가장 효과적으로 실현할 수 있는 방법입니다.
